{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pulsar pytorch elmo",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCFU5NbcG5on",
        "colab_type": "code",
        "outputId": "dc23a727-949e-41f6-945f-559c41b6bee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "additional_stops = ['dow','jones','newswires','news','wsj','com','www','http']\n",
        "stop_words = set(stopwords.words('english')+additional_stops)\n",
        "\n",
        "\n",
        "MAX_LEN = 200\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "def parse(x):\n",
        "\treturn datetime.strptime(x, '%Y-%m-%d %H:%M:%S %Z')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX0R8GzJxna3",
        "colab_type": "code",
        "outputId": "49e8ea31-e502-4e9e-ecd9-a38181b299c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        }
      },
      "source": [
        "data_df = pd.read_csv(\"https://storage.googleapis.com/dj-pulsar-examples/dj-dna-news.csv\",parse_dates = ['modification_date','publication_date'], date_parser=parse)\n",
        "print('original size',data_df.shape)\n",
        "data_df = data_df[data_df['language_code']=='en'].reset_index()\n",
        "data_df['subject_codes'] = data_df['subject_codes'].fillna('no_categories')\n",
        "data_df['text'] = data_df['title'].astype(str) + \" \" + data_df['snippet'].astype(str) + ' ' + data_df['body'].astype(str)\n",
        "print('size only english',data_df.shape)\n",
        "print('rows with no text',data_df[data_df['text'].isna()].shape[0])\n",
        "data_df.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (9,24,32,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "original size (42010, 35)\n",
            "size only english (23891, 37)\n",
            "rows with no text 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>source_code</th>\n",
              "      <th>modification_date</th>\n",
              "      <th>an</th>\n",
              "      <th>enrichment_level</th>\n",
              "      <th>art</th>\n",
              "      <th>revision_number</th>\n",
              "      <th>body</th>\n",
              "      <th>action</th>\n",
              "      <th>publication_date</th>\n",
              "      <th>credit</th>\n",
              "      <th>byline</th>\n",
              "      <th>document_type</th>\n",
              "      <th>language_code</th>\n",
              "      <th>snippet</th>\n",
              "      <th>title</th>\n",
              "      <th>copyright</th>\n",
              "      <th>dateline</th>\n",
              "      <th>region_of_origin</th>\n",
              "      <th>publisher_name</th>\n",
              "      <th>word_count</th>\n",
              "      <th>company_codes</th>\n",
              "      <th>subject_codes</th>\n",
              "      <th>region_codes</th>\n",
              "      <th>industry_codes</th>\n",
              "      <th>person_codes</th>\n",
              "      <th>currency_codes</th>\n",
              "      <th>market_index_codes</th>\n",
              "      <th>industry_classification_benchmark_codes</th>\n",
              "      <th>newswires_codes</th>\n",
              "      <th>restrictor_codes</th>\n",
              "      <th>content_type_codes</th>\n",
              "      <th>org_type_codes</th>\n",
              "      <th>footprint_company_codes</th>\n",
              "      <th>footprint_person_codes</th>\n",
              "      <th>attrib_code</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>B</td>\n",
              "      <td>2016-11-01 08:22:28</td>\n",
              "      <td>B000000020160625ec6r00015</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>More than half of zinc production goes into co...</td>\n",
              "      <td>add</td>\n",
              "      <td>2016-06-27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>By Rhiannon Hoyle</td>\n",
              "      <td>article</td>\n",
              "      <td>en</td>\n",
              "      <td>A global shortage of zinc is galvanizing inves...</td>\n",
              "      <td>As Metals Rally, Think Zinc</td>\n",
              "      <td>(c) 2016 Dow Jones &amp; Company, Inc.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NAMZ USA</td>\n",
              "      <td>Dow Jones &amp; Company, Inc.</td>\n",
              "      <td>576</td>\n",
              "      <td>march,march,sg,sg,wesah,wesah,wrlste</td>\n",
              "      <td>mnonfr,mzinc,c21,e1111,m14,ncolu,ccat,e11,ecat...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i211,ibasicm,imet</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>n/brns,n/cmd,n/djwi,n/met,n/znc,m/nnd,p/gma,j/...</td>\n",
              "      <td>fspNorm0200,B,fspFileId1016538113,FUT,CLNTALL,...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13733636,14296734,27058144,27129555,43054121,5...</td>\n",
              "      <td>34044509,34984704,35809769</td>\n",
              "      <td>B</td>\n",
              "      <td>As Metals Rally, Think Zinc A global shortage ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>2016-11-01 08:20:51</td>\n",
              "      <td>B000000020160625ec6r0000k</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>Bruce Geller, 46, has worked at Dalton Greiner...</td>\n",
              "      <td>rep</td>\n",
              "      <td>2016-06-27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>By Leslie P. Norton</td>\n",
              "      <td>article</td>\n",
              "      <td>en</td>\n",
              "      <td>[An interview with Bruce Geller\\n\\nCEO, Dalton...</td>\n",
              "      <td>The Beauty Of Microcap Stocks</td>\n",
              "      <td>(c) 2016 Dow Jones &amp; Company, Inc.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NAMZ USA</td>\n",
              "      <td>Dow Jones &amp; Company, Inc.</td>\n",
              "      <td>2122</td>\n",
              "      <td>apostl,apostl,arkres,arkres,bobev,bobev,cmgin,...</td>\n",
              "      <td>neqac,niex,nitv,reqris,ncat,nfact,nfcpex,redit...</td>\n",
              "      <td>usa,eurz,namz</td>\n",
              "      <td>i8150211,iinv,i81502,ifinal</td>\n",
              "      <td>15429719,15429719,41029749,41029749</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>dghmx,g/igv,g/ukgv,n/brns,n/cnw,n/djwi,n/stk,n...</td>\n",
              "      <td>fspNorm0200,B,fspFileId1016537770,FUT,CLNTALL,...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13625124,21314481,27076292,27112510,27198829,2...</td>\n",
              "      <td>37792663,41926582,439523,87182973</td>\n",
              "      <td>B</td>\n",
              "      <td>The Beauty Of Microcap Stocks [An interview wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>B</td>\n",
              "      <td>2016-11-01 08:18:30</td>\n",
              "      <td>B000000020160625ec6r0000b</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>The back-to-back boost in home sales only deep...</td>\n",
              "      <td>add</td>\n",
              "      <td>2016-06-27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>By Gene Epstein</td>\n",
              "      <td>article</td>\n",
              "      <td>en</td>\n",
              "      <td>The housing market is finally advancing at a w...</td>\n",
              "      <td>Housing: No Bubble, No Bust</td>\n",
              "      <td>(c) 2016 Dow Jones &amp; Company, Inc.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NAMZ USA</td>\n",
              "      <td>Dow Jones &amp; Company, Inc.</td>\n",
              "      <td>602</td>\n",
              "      <td>euruno</td>\n",
              "      <td>e1121,e11,ecat,ehsal,ncolu,ereal,ncat</td>\n",
              "      <td>eurz,usa,namz</td>\n",
              "      <td>NaN</td>\n",
              "      <td>110608003,110608003</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>g/igv,g/ukgv,n/brns,n/djwi,n/eco,n/geni,n/vot,...</td>\n",
              "      <td>fspNorm0200,B,fspFileId1016537481,FUT,CLNTALL,...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>27068499,27112510,43054121,56918265</td>\n",
              "      <td>33929114</td>\n",
              "      <td>B</td>\n",
              "      <td>Housing: No Bubble, No Bust The housing market...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>B</td>\n",
              "      <td>2016-11-01 08:20:15</td>\n",
              "      <td>B000000020160625ec6r0000y</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>The upside potential for the person selling th...</td>\n",
              "      <td>rep</td>\n",
              "      <td>2016-06-27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>By Karen Hube</td>\n",
              "      <td>article</td>\n",
              "      <td>en</td>\n",
              "      <td>The $2.7 trillion annuity industry's latest go...</td>\n",
              "      <td>Special Report: Retirement: The Top 50 Annuities</td>\n",
              "      <td>(c) 2016 Dow Jones &amp; Company, Inc.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NAMZ USA</td>\n",
              "      <td>Dow Jones &amp; Company, Inc.</td>\n",
              "      <td>1543</td>\n",
              "      <td>allanz,allanz,nyklif,nyklif,sactur,stndpr,allanz</td>\n",
              "      <td>gretir,gpersf,npag,gcat,ncat</td>\n",
              "      <td>NaN</td>\n",
              "      <td>iannui,i81502,i82,i82002,ifinal,iinv</td>\n",
              "      <td>109894890,109894890,79176154,79176154</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>n/brns,n/djwi,n/pfn,m/nnd,p/wmai,j/cst</td>\n",
              "      <td>fspNorm0200,B,fspFileId1016538045,FUT,CLNTALL,...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>co,pub</td>\n",
              "      <td>11767142,27060787,27065622,27085630,27173990,2...</td>\n",
              "      <td>128427,36142751,44231489,48105120,72969811</td>\n",
              "      <td>B</td>\n",
              "      <td>Special Report: Retirement: The Top 50 Annuiti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>B</td>\n",
              "      <td>2016-11-01 08:21:36</td>\n",
              "      <td>B000000020160625ec6r00011</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>Armstrong, with leading market shares in most ...</td>\n",
              "      <td>add</td>\n",
              "      <td>2016-06-27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>By David Englander</td>\n",
              "      <td>article</td>\n",
              "      <td>en</td>\n",
              "      <td>A month ago, this column weighed in with a pos...</td>\n",
              "      <td>Solid Footings for Growth</td>\n",
              "      <td>(c) 2016 Dow Jones &amp; Company, Inc.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NAMZ USA</td>\n",
              "      <td>Dow Jones &amp; Company, Inc.</td>\n",
              "      <td>894</td>\n",
              "      <td>armind,armind,armind,homd,homd,lowco,lowco,armhi</td>\n",
              "      <td>c01,c152,cspin,ncolu,neqac,c02,c11,c15,c18,cac...</td>\n",
              "      <td>usa,namz</td>\n",
              "      <td>i2412,ibuildpr,icre</td>\n",
              "      <td>31374,31374,81764492,81764492</td>\n",
              "      <td>NaN</td>\n",
              "      <td>xdjiic,xnyci,xr3000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>afi,i/bld,i/xisl,i/xnya,i/xrus,n/brns,n/cac,n/...</td>\n",
              "      <td>fspNorm0200,B,fspFileId1016538076,FUT,CLNTALL,...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>co,pub</td>\n",
              "      <td>13461163,27899804,42730824,43054121</td>\n",
              "      <td>110972771,34363694,34508252,46437587,485218</td>\n",
              "      <td>B</td>\n",
              "      <td>Solid Footings for Growth A month ago, this co...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ...                                               text\n",
              "0      0  ...  As Metals Rally, Think Zinc A global shortage ...\n",
              "1      1  ...  The Beauty Of Microcap Stocks [An interview wi...\n",
              "2      2  ...  Housing: No Bubble, No Bust The housing market...\n",
              "3      3  ...  Special Report: Retirement: The Top 50 Annuiti...\n",
              "4      4  ...  Solid Footings for Growth A month ago, this co...\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk93PRZ-YFGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W7Bqk4ZxzDH",
        "colab_type": "code",
        "outputId": "8c74096e-b3e3-4a18-e4b4-f616eab8975f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''cols_ignore = ['text','body','snippet','title','byline','person_codes','art']\n",
        "\n",
        "for col in set(data_df.columns)-set(cols_ignore):\n",
        "  col_vals = data_df[col].value_counts()\n",
        "  print(col,': ',len(col_vals))\n",
        "  if np.issubdtype(data_df[col].dtype, np.number) or col in cols_ignore:\n",
        "    continue\n",
        "  elif len(col_vals) < 20:\n",
        "    fig, ax = plt.subplots(figsize=(10,7))\n",
        "    col_vals.sort_index().plot.bar()\n",
        "    plt.legend([col],loc='best')\n",
        "    plt.title(col + ' len: ' + str(len(col_vals)))\n",
        "    fig.show()\n",
        "    #print('\\t',' '.join([str(x) + \" | \" for x in data_df[col].value_counts().index]),'\\n')\n",
        "  else:\n",
        "    for item in col_vals.sort_index()[0:3].index:\n",
        "      print('\\t',item)\n",
        "    #print('\\t',' '.join([str(x) + \" | \" for x in data_df[col].value_counts()[0:10].index]),'\\n')\n",
        "  '''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cols_ignore = [\\'text\\',\\'body\\',\\'snippet\\',\\'title\\',\\'byline\\',\\'person_codes\\',\\'art\\']\\n\\nfor col in set(data_df.columns)-set(cols_ignore):\\n  col_vals = data_df[col].value_counts()\\n  print(col,\\': \\',len(col_vals))\\n  if np.issubdtype(data_df[col].dtype, np.number) or col in cols_ignore:\\n    continue\\n  elif len(col_vals) < 20:\\n    fig, ax = plt.subplots(figsize=(10,7))\\n    col_vals.sort_index().plot.bar()\\n    plt.legend([col],loc=\\'best\\')\\n    plt.title(col + \\' len: \\' + str(len(col_vals)))\\n    fig.show()\\n    #print(\\'\\t\\',\\' \\'.join([str(x) + \" | \" for x in data_df[col].value_counts().index]),\\'\\n\\')\\n  else:\\n    for item in col_vals.sort_index()[0:3].index:\\n      print(\\'\\t\\',item)\\n    #print(\\'\\t\\',\\' \\'.join([str(x) + \" | \" for x in data_df[col].value_counts()[0:10].index]),\\'\\n\\')\\n  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrcIQVmfSzWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVJ5cUzFlpQt",
        "colab_type": "code",
        "outputId": "773b4bee-1be1-44c2-e954-0a1047db6423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''import seaborn as sns\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "\n",
        "for col in ['title','snippet','body']:\n",
        "\n",
        "  try:\n",
        "    sentence_lengths = [len(x) for x in data_df[col].astype(str).str.split()]\n",
        "  except:\n",
        "    print(col)\n",
        "    continue\n",
        "  fig, ax = plt.subplots(figsize=(20,7))\n",
        "  f = sns.distplot(sentence_lengths , fit=norm);\n",
        "  f.figure.set_size_inches(15,7)\n",
        "  \n",
        "  (mu, sigma) = norm.fit(sentence_lengths)\n",
        "  print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
        "  plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
        "              loc='best')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.xlim(0, mu + sigma * 1.5)\n",
        "  plt.title(col + ' distribution')\n",
        "  fig.show()'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import seaborn as sns\\nfrom scipy.stats import norm\\nfrom scipy import stats\\n\\nfor col in ['title','snippet','body']:\\n\\n  try:\\n    sentence_lengths = [len(x) for x in data_df[col].astype(str).str.split()]\\n  except:\\n    print(col)\\n    continue\\n  fig, ax = plt.subplots(figsize=(20,7))\\n  f = sns.distplot(sentence_lengths , fit=norm);\\n  f.figure.set_size_inches(15,7)\\n  \\n  (mu, sigma) = norm.fit(sentence_lengths)\\n  print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\\n  plt.legend(['Normal dist. ($\\\\mu=$ {:.2f} and $\\\\sigma=$ {:.2f} )'.format(mu, sigma)],\\n              loc='best')\\n  plt.ylabel('Frequency')\\n  plt.xlim(0, mu + sigma * 1.5)\\n  plt.title(col + ' distribution')\\n  fig.show()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PlPtyG7n7oi",
        "colab_type": "text"
      },
      "source": [
        "Ave sentence length 400 words.  Might need to go with a long text sequence length.  May also require removal of many stop words. Or removal title and snippets or only use those.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnVtmZptCUsb",
        "colab_type": "code",
        "outputId": "a4955c0c-236d-4cbe-dd13-a365267f844a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "data_df.subject_codes.value_counts()[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "c1522,cstdr,neqac,c15,cactio,ccat,ncat,nfact,nfce,nfcpin                            1397\n",
              "gspo,ncat,nfact,nfce,nrgn                                                           1008\n",
              "c151,neqac,c15,ccat,ncat,nfact,nfcpin                                                929\n",
              "neqac,ncat,nfact                                                                     755\n",
              "no_categories                                                                        588\n",
              "e1202,m12,m131,ncat,neqac,nfiac,ntab,e12,ecat,m13,mcat,nfact,nfce,niwe               488\n",
              "cdirdl,cffil,cgvfil,neqac,c18,c41,cactio,ccat,ncat,nfact,nfcpex,nfcpin               441\n",
              "cdirdl,cgvfil,cissal,neqac,c17,c171,c18,c41,cactio,ccat,ncat,nfact,nfcpex,nfcpin     260\n",
              "c174,neqac,nfiac,c17,c172,cactio,ccat,ncat,nfact,nfcpin                              211\n",
              "gsocc,gspo,ncat,nfact,nfce,nrgn                                                      197\n",
              "Name: subject_codes, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvXOiU3RCkvV",
        "colab_type": "text"
      },
      "source": [
        "Dataset is heavily skewed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZiGdWYEYY1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(\"\\'\", \"\", text)\n",
        "    #text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', text)\n",
        "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
        "    #text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "    text = ' '.join([w for w in text.split()[:MAX_LEN] if not w in stop_words])\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "data_df['clean_text'] = data_df['text'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7sa_EG3X9jb",
        "colab_type": "code",
        "outputId": "a1786bc5-2a72-471b-d4dc-d92dbdab381d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "text_2d = np.array(data_df['text'].str.split())\n",
        "text_2d.shape\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_rus, y_rus = rus.fit_resample(text_2d, labels_matrix)\n",
        "X_rus.shape, y_rus.shape\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros = RandomOverSampler(sampling_strategy='not majority',random_state=42)\n",
        "X_ros, y_ros = ros.fit_resample(data_df[['text']], labels_matrix)\n",
        "X_ros.shape, y_ros.shape\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntext_2d = np.array(data_df['text'].str.split())\\ntext_2d.shape\\n\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\nrus = RandomUnderSampler(random_state=42)\\nX_rus, y_rus = rus.fit_resample(text_2d, labels_matrix)\\nX_rus.shape, y_rus.shape\\n\\nfrom imblearn.over_sampling import RandomOverSampler\\n\\nros = RandomOverSampler(sampling_strategy='not majority',random_state=42)\\nX_ros, y_ros = ros.fit_resample(data_df[['text']], labels_matrix)\\nX_ros.shape, y_ros.shape\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rd6WwbLK3O8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df['subject_codes'] = data_df['subject_codes'].fillna('nocategory')\n",
        "data_df['subject_codes_list'] = data_df['subject_codes'].str.split(',')   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77G93OhMekr4",
        "colab_type": "code",
        "outputId": "02555b7a-2a81-4206-ad5a-4bea42abc855",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "multilabel_binarizer = MultiLabelBinarizer()\n",
        "multilabel_binarizer.fit(data_df['subject_codes_list'])\n",
        "y = multilabel_binarizer.transform(data_df['subject_codes_list'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiLabelBinarizer(classes=None, sparse_output=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvBPrReEn5CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_val,y_train,y_val = train_test_split(data_df['clean_text'],y,test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyX0NhRP0Xgi",
        "colab_type": "code",
        "outputId": "3fbfc520-0705-4c79-8a7e-bb7bd95d64c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install overrides\n",
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (1.9)\n",
            "Requirement already satisfied: allennlp in /usr/local/lib/python3.6/dist-packages (0.8.4)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.10.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.8)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.8)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Requirement already satisfied: conllu==0.11 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.11)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.189)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.13.0)\n",
            "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.2)\n",
            "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.9.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp) (5.5.1)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Requirement already satisfied: spacy<2.2,>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.6)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.3)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.4)\n",
            "Requirement already satisfied: awscli>=1.11.91 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.209)\n",
            "Requirement already satisfied: flaky in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from responses>=0.7->allennlp) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.12.199)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (41.0.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (7.1.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.1.0)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.13.2)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.15.5)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.0.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.0.1)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.6.8)\n",
            "Requirement already satisfied: colorama<=0.3.9,>=0.2.5 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.3.9)\n",
            "Requirement already satisfied: PyYAML<=5.1,>=3.10; python_version != \"2.6\" in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
            "Requirement already satisfied: rsa<=3.5.0,>=3.1.2 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.4.2)\n",
            "Requirement already satisfied: docutils<0.15,>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.9.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dymv6iD0QVS",
        "colab_type": "code",
        "outputId": "bb9453a3-1389-4273-f749-b921cd20793a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from typing import Iterator, List, Dict,TypeVar, Generic\n",
        "\n",
        "import logging, csv,re\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "from tqdm.auto import tqdm\n",
        "from overrides import overrides\n",
        "from allennlp.data import Instance\n",
        "from allennlp.data.dataset_readers import DatasetReader\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.tokenizers import Tokenizer, WordTokenizer\n",
        "from allennlp.data.tokenizers.word_splitter import JustSpacesWordSplitter\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.data.fields import LabelField, TextField, Field, SequenceLabelField, MetadataField, ArrayField\n",
        "from allennlp.data.fields.multilabel_field import MultiLabelField\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
        "from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper,CnnEncoder,BagOfEmbeddingsEncoder\n",
        "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder\n",
        "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "\n",
        "\n",
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "from allennlp.predictors import SentenceTaggerPredictor\n",
        "from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\n",
        "from allennlp.modules.token_embedders import ElmoTokenEmbedder\n",
        "\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
        "from allennlp.models import Model\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "EMBEDDING_DIM = 128\n",
        "\n",
        "num_labels = 690"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l8JzQlc0bUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#labels_list = []\n",
        "non_string_ixs = []\n",
        "nan_labels_ixs = []\n",
        "\n",
        "class MultilabelDatasetReader(DatasetReader):\n",
        "    def __init__(self,\n",
        "                 lazy: bool = False,\n",
        "                 tokenizer: Tokenizer = None,\n",
        "                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
        "        super().__init__(lazy)\n",
        "        self._tokenizer = tokenizer or WordTokenizer(JustSpacesWordSplitter())\n",
        "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
        "\n",
        "    @overrides\n",
        "    def _read(self, Xy):\n",
        "        logger.info(\"Reading instances from lines in df\")\n",
        "        for x_i, y_i in zip(Xy[0],Xy[1]):\n",
        "          if type(x_i) == str:\n",
        "            review_text = clean_text(x_i)\n",
        "          else:\n",
        "            non_string_ixs.append(i)\n",
        "            continue\n",
        "          yield self.text_to_instance(row_ix=x_i.index,text=review_text, labels=y_i)\n",
        "\n",
        "    @overrides\n",
        "    def text_to_instance(self,  # type: ignore\n",
        "                         row_ix: int,\n",
        "                         text: str,\n",
        "                         labels: [str] = None) -> Instance:\n",
        "        fields: Dict[str, Field] = {}\n",
        "        tokenized_text = self._tokenizer.tokenize(text)\n",
        "        fields[\"tokens\"] = TextField(tokenized_text[:MAX_LEN], self._token_indexers)\n",
        "        if labels is None:\n",
        "            labels = np.zeros(len(label_cols))      \n",
        "        label_field = ArrayField(array=labels)\n",
        "        fields[\"label\"] = label_field\n",
        "        return Instance(fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iROqxGeS0cCX",
        "colab_type": "code",
        "outputId": "50417d40-9f85-4ce3-95e0-7f8263fd39c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "TokenType = TypeVar(\"TokenType\", int, torch.Tensor)  # List[int])\n",
        "\n",
        "elmo_token_indexer = ELMoTokenCharactersIndexer(TokenIndexer[torch.Tensor])\n",
        "reader = MultilabelDatasetReader(\n",
        "    token_indexers={'tokens': elmo_token_indexer})\n",
        "\n",
        "\n",
        "Xy_train = (x_train,y_train)\n",
        "Xy_val = (x_val,y_val)\n",
        "train_dataset = reader.read(Xy_train)\n",
        "dev_dataset = reader.read(Xy_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19112it [00:09, 1962.54it/s]\n",
            "4779it [00:02, 1958.04it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avvCcZnq0cGr",
        "colab_type": "code",
        "outputId": "14037d6f-99c8-4642-aef2-4d129f71797b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "options_file = ('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo'\n",
        "                '/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json')\n",
        "weight_file = ('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo'\n",
        "               '/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5')\n",
        "\n",
        "elmo_embedder = ElmoTokenEmbedder(options_file, weight_file)\n",
        "\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": elmo_embedder})\n",
        "\n",
        "vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
        "                                  min_count={'tokens': 3})\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 23891/23891 [00:00<00:00, 66800.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XbyBGgmG23E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6rYHIifZPjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class LayeredBOEClassifier(Model):\n",
        "    def __init__(self,\n",
        "                 embedder: TextFieldEmbedder,\n",
        "                 embedding_size: int,\n",
        "                 vocab: Vocabulary) -> None:\n",
        "        super().__init__(vocab)\n",
        "        self.embedder = embedder\n",
        "\n",
        "        self.encoder = BagOfEmbeddingsEncoder(embedding_size)\n",
        "\n",
        "        self.linearIn = torch.nn.Linear(in_features=self.encoder.get_output_dim(),\n",
        "                                      out_features=128)\n",
        "        self.linearMid = torch.nn.Linear(in_features=128,\n",
        "                                      out_features=128)\n",
        "        \n",
        "        self.linearOut = torch.nn.Linear(in_features=128,\n",
        "                                      out_features=num_labels)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.relu2 = torch.nn.ReLU()\n",
        "        self.loss_function = torch.nn.BCEWithLogitsLoss()   #CrossEntropyLoss()  #BCELoss()\n",
        "         \n",
        "\n",
        "    def forward(self,\n",
        "                tokens: Dict[str, torch.Tensor],\n",
        "                label: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
        "        mask = get_text_field_mask(tokens)\n",
        "\n",
        "        embeddings = self.embedder(tokens)\n",
        "        encoder_out = self.encoder(embeddings, mask)\n",
        "        #print(\"encoder_out shape \",encoder_out.shape) #= 256\n",
        "        x = self.linearIn(encoder_out)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.linearMid(x)\n",
        "        x = self.relu2(x)\n",
        "        logits = self.linearOut(x)\n",
        "        \n",
        "        #logits = torch.sigmoid(logits)\n",
        "        logits_hist.append(logits)\n",
        "        output = {\"logits\": logits}\n",
        "        output[\"loss\"] = self.loss_function(logits, label)\n",
        "        train_loss_hist.append(output[\"loss\"])\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCkbh0Hx7am_",
        "colab_type": "code",
        "outputId": "31b6cb65-2a2c-4a93-f572-25e4916d415a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = 0 if torch.cuda.is_available() else 1\n",
        "print(device)\n",
        "\n",
        "elmo_embedding_dim = 256\n",
        "HIDDEN_DIM = 64 #128\n",
        "\n",
        "#num_filters = 1\n",
        "num_epochs = 1\n",
        "#dropout = 0.25\n",
        "#layers = 2\n",
        "\n",
        "#model = SimpleClassifier(word_embeddings, vocab, elmo_embedding_dim, HIDDEN_DIM, layers, dropout).cuda()\n",
        "model = LayeredBOEClassifier(word_embeddings, elmo_embedding_dim, vocab).cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0001, weight_decay=1e-6)\n",
        "\n",
        "iterator = BucketIterator(batch_size=16, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
        "iterator.index_with(vocab)\n",
        "\n",
        "trainer = Trainer(model=model,\n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=dev_dataset,\n",
        "                  cuda_device=device,\n",
        "                  patience=1,\n",
        "                  num_epochs=num_epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dek6cPvh8Wkb",
        "colab_type": "code",
        "outputId": "da760c6f-102e-4b38-b4d1-edeb1a765c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "temp_hist = []\n",
        "\n",
        "logits_hist = []\n",
        "labels_hist = []\n",
        "train_loss_hist = []\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 0.0347 ||: 100%|██████████| 1195/1195 [03:04<00:00,  6.89it/s]\n",
            "loss: 0.0228 ||: 100%|██████████| 299/299 [00:44<00:00,  5.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 0,\n",
              " 'best_validation_loss': 0.02275467050700981,\n",
              " 'epoch': 0,\n",
              " 'peak_cpu_memory_MB': 4246.432,\n",
              " 'peak_gpu_0_memory_MB': 835,\n",
              " 'training_cpu_memory_MB': 4246.432,\n",
              " 'training_duration': '0:03:48.629647',\n",
              " 'training_epochs': 0,\n",
              " 'training_gpu_0_memory_MB': 835,\n",
              " 'training_loss': 0.034694792786379304,\n",
              " 'training_start_epoch': 0,\n",
              " 'validation_loss': 0.02275467050700981}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1uDt8KbYsU0",
        "colab_type": "code",
        "outputId": "1571a762-1599-49f1-b9e9-60994055f1db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "plt.title('Cost after training x epochs')\n",
        "plt.xlabel('No. of iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.plot(train_loss_hist)\n",
        "\n",
        "plt.legend([\"train\",\"val\"],\n",
        "            loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Cost after training x epochs')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'No. of iterations')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Cost')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa13a1de1d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa13a1de5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICIiMf8PHZ/oU6V9maEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5g2p3i0UpEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moUYUY1EFYYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.iterators import DataIterator\n",
        "from tqdm import tqdm\n",
        "from scipy.special import expit # the sigmoid function\n",
        "from allennlp.nn import util as nn_util\n",
        "\n",
        "def tonp(tsr): return tsr.detach().cpu().numpy()\n",
        "\n",
        "class Predictor:\n",
        "    def __init__(self, model: Model, iterator: DataIterator,\n",
        "                 cuda_device: int=-1) -> None:\n",
        "        self.model = model\n",
        "        self.iterator = iterator\n",
        "        self.cuda_device = cuda_device\n",
        "        \n",
        "    def _extract_data(self, batch) -> np.ndarray:\n",
        "        out_dict = self.model(**batch)\n",
        "        return expit(tonp(out_dict[\"logits\"]))\n",
        "    \n",
        "    def predict(self, ds) -> np.ndarray:\n",
        "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
        "        self.model.eval()\n",
        "        pred_generator_tqdm = tqdm(pred_generator,\n",
        "                                   total=self.iterator.get_num_batches(ds))\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for batch in pred_generator_tqdm:\n",
        "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
        "                preds.append(self._extract_data(batch))\n",
        "        return np.concatenate(preds, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q68exe5J89Xq",
        "colab_type": "code",
        "outputId": "833667e5-d4b0-4102-e400-e2a02dced441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from allennlp.data.iterators import BasicIterator\n",
        "# iterate over the dataset without changing its order\n",
        "seq_iterator = BasicIterator(batch_size=64)\n",
        "seq_iterator.index_with(vocab)\n",
        "\n",
        "predictor = Predictor(model, seq_iterator, cuda_device=0)\n",
        "#train_preds = predictor.predict(train_dataset) \n",
        "test_preds = predictor.predict(dev_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [00:21<00:00,  3.69it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBAfoCsqliM4",
        "colab_type": "code",
        "outputId": "56a43139-9466-467f-e270-3868dc748e9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_preds.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4779, 690)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a7WckONEURJ",
        "colab_type": "code",
        "outputId": "175ed973-1413-41ba-fd5d-ad30b00addbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "all_cats = []\n",
        "\n",
        "for pred in (test_preds-.5)*2:\n",
        "  cats= []\n",
        "  for i,val in enumerate(pred):\n",
        "    if val > .3:\n",
        "      cats.append(i)\n",
        "  all_cats.append(cats)\n",
        "\n",
        "all_cats[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[453, 592, 605, 606, 648],\n",
              " [9, 10, 66, 592, 604, 605, 608],\n",
              " [52, 53, 66, 144, 592, 604, 605, 608],\n",
              " [518, 529, 586, 592, 604, 605, 606, 609, 610, 611],\n",
              " [19, 25, 515, 529, 550, 557, 592, 604, 605, 609]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHGtDIQjg9xX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKsx-9SfjCJN",
        "colab_type": "code",
        "outputId": "7602f6a6-9dd8-4a00-94ac-a7269097ae53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "test_preds_binary = (test_preds >= .3).astype(int)\n",
        "f1_score(y_val, test_preds_binary, average=\"micro\")\n",
        "\n",
        "print(classification_report(y_val, test_preds_binary))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7202490556009037"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.58      0.69      0.63       238\n",
            "           2       0.00      0.00      0.00        96\n",
            "           3       0.53      0.54      0.54       123\n",
            "           4       0.33      0.16      0.22       105\n",
            "           5       0.00      0.00      0.00        10\n",
            "           6       0.00      0.00      0.00         5\n",
            "           7       1.00      0.20      0.33        15\n",
            "           8       0.00      0.00      0.00        31\n",
            "           9       0.91      0.71      0.80      1087\n",
            "          10       0.96      0.60      0.74       500\n",
            "          11       0.80      0.22      0.35        18\n",
            "          12       0.00      0.00      0.00        68\n",
            "          13       0.00      0.00      0.00        39\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.55      0.21      0.30       242\n",
            "          16       0.65      0.10      0.18       214\n",
            "          17       0.81      0.87      0.84       387\n",
            "          18       0.00      0.00      0.00        19\n",
            "          19       0.71      0.65      0.68       505\n",
            "          20       0.87      0.71      0.78       246\n",
            "          21       0.00      0.00      0.00        27\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.80      0.48      0.60       200\n",
            "          24       0.56      0.32      0.41        87\n",
            "          25       0.79      0.62      0.70       164\n",
            "          26       0.62      0.85      0.72       764\n",
            "          27       0.45      0.71      0.55       408\n",
            "          28       0.00      0.00      0.00        35\n",
            "          29       0.00      0.00      0.00        15\n",
            "          30       0.00      0.00      0.00        10\n",
            "          31       0.29      0.17      0.21        30\n",
            "          32       0.75      0.46      0.57        13\n",
            "          33       0.00      0.00      0.00        36\n",
            "          34       0.00      0.00      0.00         4\n",
            "          35       0.00      0.00      0.00        14\n",
            "          36       0.00      0.00      0.00        92\n",
            "          37       0.00      0.00      0.00         0\n",
            "          38       0.00      0.00      0.00         7\n",
            "          39       0.00      0.00      0.00         7\n",
            "          40       0.00      0.00      0.00        27\n",
            "          41       0.00      0.00      0.00         1\n",
            "          42       0.00      0.00      0.00         2\n",
            "          43       0.00      0.00      0.00        19\n",
            "          44       0.07      0.03      0.04        34\n",
            "          45       0.00      0.00      0.00         1\n",
            "          46       0.00      0.00      0.00         3\n",
            "          47       0.00      0.00      0.00        22\n",
            "          48       0.00      0.00      0.00         1\n",
            "          49       0.00      0.00      0.00         3\n",
            "          50       0.00      0.00      0.00        15\n",
            "          51       0.00      0.00      0.00         3\n",
            "          52       0.76      0.83      0.79       508\n",
            "          53       0.48      0.58      0.53       103\n",
            "          54       0.00      0.00      0.00        39\n",
            "          55       0.44      0.57      0.49       194\n",
            "          56       0.74      0.84      0.79      1460\n",
            "          57       0.00      0.00      0.00         0\n",
            "          58       0.00      0.00      0.00         0\n",
            "          59       0.00      0.00      0.00         5\n",
            "          60       0.00      0.00      0.00         6\n",
            "          61       0.17      0.14      0.16        28\n",
            "          62       0.00      0.00      0.00        23\n",
            "          63       0.00      0.00      0.00         0\n",
            "          64       0.00      0.00      0.00         0\n",
            "          65       0.00      0.00      0.00         4\n",
            "          66       0.83      0.89      0.86      2567\n",
            "          67       0.00      0.00      0.00         2\n",
            "          68       0.00      0.00      0.00         0\n",
            "          69       0.00      0.00      0.00        20\n",
            "          70       0.00      0.00      0.00         5\n",
            "          71       0.00      0.00      0.00         2\n",
            "          72       0.00      0.00      0.00         0\n",
            "          73       0.00      0.00      0.00        12\n",
            "          74       0.00      0.00      0.00         4\n",
            "          75       0.00      0.00      0.00         3\n",
            "          76       0.00      0.00      0.00         1\n",
            "          77       0.00      0.00      0.00         1\n",
            "          78       0.00      0.00      0.00         2\n",
            "          79       0.00      0.00      0.00         2\n",
            "          80       0.00      0.00      0.00         2\n",
            "          81       0.00      0.00      0.00         1\n",
            "          82       0.94      0.93      0.93       335\n",
            "          83       0.00      0.00      0.00         1\n",
            "          84       0.00      0.00      0.00        24\n",
            "          85       0.00      0.00      0.00         0\n",
            "          86       0.00      0.00      0.00         1\n",
            "          87       0.00      0.00      0.00         2\n",
            "          88       0.00      0.00      0.00         0\n",
            "          89       0.00      0.00      0.00         1\n",
            "          90       0.00      0.00      0.00         4\n",
            "          91       0.00      0.00      0.00         3\n",
            "          92       0.00      0.00      0.00         0\n",
            "          93       1.00      0.98      0.99       120\n",
            "          94       0.00      0.00      0.00         1\n",
            "          95       0.00      0.00      0.00         1\n",
            "          96       0.79      0.96      0.87       419\n",
            "          97       0.00      0.00      0.00        15\n",
            "          98       0.00      0.00      0.00         4\n",
            "          99       0.00      0.00      0.00         1\n",
            "         100       0.00      0.00      0.00         3\n",
            "         101       0.00      0.00      0.00         6\n",
            "         102       0.00      0.00      0.00        15\n",
            "         103       0.00      0.00      0.00         2\n",
            "         104       0.00      0.00      0.00         2\n",
            "         105       0.00      0.00      0.00         0\n",
            "         106       0.00      0.00      0.00        63\n",
            "         107       0.62      0.98      0.76       107\n",
            "         108       0.00      0.00      0.00         0\n",
            "         109       0.00      0.00      0.00         3\n",
            "         110       0.00      0.00      0.00         1\n",
            "         111       0.00      0.00      0.00         1\n",
            "         112       0.00      0.00      0.00         6\n",
            "         113       0.00      0.00      0.00         3\n",
            "         114       0.16      0.26      0.20        34\n",
            "         115       0.00      0.00      0.00         5\n",
            "         116       0.00      0.00      0.00         4\n",
            "         117       0.00      0.00      0.00        15\n",
            "         118       0.00      0.00      0.00         2\n",
            "         119       0.00      0.00      0.00         2\n",
            "         120       0.00      0.00      0.00         3\n",
            "         121       0.00      0.00      0.00         0\n",
            "         122       0.00      0.00      0.00         6\n",
            "         123       0.00      0.00      0.00        16\n",
            "         124       0.00      0.00      0.00         9\n",
            "         125       0.00      0.00      0.00         0\n",
            "         126       0.00      0.00      0.00         2\n",
            "         127       0.00      0.00      0.00         2\n",
            "         128       0.00      0.00      0.00         6\n",
            "         129       0.00      0.00      0.00         5\n",
            "         130       0.00      0.00      0.00         3\n",
            "         131       0.00      0.00      0.00         3\n",
            "         132       0.00      0.00      0.00         4\n",
            "         133       0.00      0.00      0.00         2\n",
            "         134       0.00      0.00      0.00         2\n",
            "         135       0.00      0.00      0.00         0\n",
            "         136       0.00      0.00      0.00         5\n",
            "         137       0.00      0.00      0.00         0\n",
            "         138       0.73      0.96      0.83       135\n",
            "         139       0.00      0.00      0.00         0\n",
            "         140       0.00      0.00      0.00         1\n",
            "         141       0.00      0.00      0.00         7\n",
            "         142       0.00      0.00      0.00         3\n",
            "         143       0.00      0.00      0.00        22\n",
            "         144       0.45      0.64      0.53       160\n",
            "         145       0.00      0.00      0.00         8\n",
            "         146       0.00      0.00      0.00        15\n",
            "         147       0.00      0.00      0.00         2\n",
            "         148       0.84      0.97      0.90       346\n",
            "         149       0.00      0.00      0.00         6\n",
            "         150       0.00      0.00      0.00         2\n",
            "         151       0.00      0.00      0.00         7\n",
            "         152       0.00      0.00      0.00         2\n",
            "         153       0.00      0.00      0.00         0\n",
            "         154       1.00      0.14      0.25         7\n",
            "         155       0.00      0.00      0.00         4\n",
            "         156       0.00      0.00      0.00         3\n",
            "         157       0.00      0.00      0.00         0\n",
            "         158       0.00      0.00      0.00         1\n",
            "         159       0.00      0.00      0.00         5\n",
            "         160       0.34      0.50      0.40       130\n",
            "         161       0.00      0.00      0.00        19\n",
            "         162       0.00      0.00      0.00        15\n",
            "         163       0.00      0.00      0.00        16\n",
            "         164       0.00      0.00      0.00         2\n",
            "         165       0.00      0.00      0.00         4\n",
            "         166       0.00      0.00      0.00         4\n",
            "         167       0.00      0.00      0.00         2\n",
            "         168       0.00      0.00      0.00         2\n",
            "         169       0.00      0.00      0.00         0\n",
            "         170       0.00      0.00      0.00        17\n",
            "         171       0.00      0.00      0.00         4\n",
            "         172       0.00      0.00      0.00         4\n",
            "         173       0.00      0.00      0.00         1\n",
            "         174       0.00      0.00      0.00        13\n",
            "         175       0.00      0.00      0.00         3\n",
            "         176       0.00      0.00      0.00         5\n",
            "         177       0.00      0.00      0.00         1\n",
            "         178       0.00      0.00      0.00         1\n",
            "         179       0.00      0.00      0.00        11\n",
            "         180       0.00      0.00      0.00         0\n",
            "         181       0.00      0.00      0.00         0\n",
            "         182       0.00      0.00      0.00         1\n",
            "         183       0.00      0.00      0.00         2\n",
            "         184       0.56      0.78      0.65       185\n",
            "         185       0.00      0.00      0.00        23\n",
            "         186       0.66      0.83      0.74       149\n",
            "         187       0.00      0.00      0.00        51\n",
            "         188       0.00      0.00      0.00        29\n",
            "         189       0.00      0.00      0.00         7\n",
            "         190       0.00      0.00      0.00         0\n",
            "         191       0.00      0.00      0.00        24\n",
            "         192       0.00      0.00      0.00         1\n",
            "         193       0.00      0.00      0.00         5\n",
            "         194       0.00      0.00      0.00        49\n",
            "         195       0.00      0.00      0.00         9\n",
            "         196       0.00      0.00      0.00        30\n",
            "         197       0.00      0.00      0.00        64\n",
            "         198       0.00      0.00      0.00         2\n",
            "         199       0.00      0.00      0.00         3\n",
            "         200       0.00      0.00      0.00         1\n",
            "         201       0.00      0.00      0.00         0\n",
            "         202       0.46      0.85      0.59       550\n",
            "         203       0.00      0.00      0.00         0\n",
            "         204       0.00      0.00      0.00         6\n",
            "         205       0.00      0.00      0.00         0\n",
            "         206       0.00      0.00      0.00         9\n",
            "         207       0.00      0.00      0.00        19\n",
            "         208       0.00      0.00      0.00         4\n",
            "         209       0.00      0.00      0.00         3\n",
            "         210       0.00      0.00      0.00         8\n",
            "         211       0.00      0.00      0.00        14\n",
            "         212       0.00      0.00      0.00         2\n",
            "         213       0.00      0.00      0.00         7\n",
            "         214       0.00      0.00      0.00        11\n",
            "         215       0.00      0.00      0.00         7\n",
            "         216       0.00      0.00      0.00        16\n",
            "         217       0.00      0.00      0.00         0\n",
            "         218       0.00      0.00      0.00         0\n",
            "         219       1.00      0.33      0.50         6\n",
            "         220       0.00      0.00      0.00         3\n",
            "         221       0.00      0.00      0.00         1\n",
            "         222       0.00      0.00      0.00         1\n",
            "         223       0.00      0.00      0.00         4\n",
            "         224       0.00      0.00      0.00         0\n",
            "         225       0.00      0.00      0.00         0\n",
            "         226       0.00      0.00      0.00         0\n",
            "         227       0.00      0.00      0.00         3\n",
            "         228       0.00      0.00      0.00         3\n",
            "         229       0.00      0.00      0.00         0\n",
            "         230       0.00      0.00      0.00         0\n",
            "         231       0.00      0.00      0.00         6\n",
            "         232       0.00      0.00      0.00         1\n",
            "         233       0.00      0.00      0.00         1\n",
            "         234       0.00      0.00      0.00         4\n",
            "         235       0.00      0.00      0.00         0\n",
            "         236       0.00      0.00      0.00         0\n",
            "         237       0.00      0.00      0.00         1\n",
            "         238       0.00      0.00      0.00         0\n",
            "         239       0.00      0.00      0.00         0\n",
            "         240       0.00      0.00      0.00         0\n",
            "         241       0.00      0.00      0.00        11\n",
            "         242       0.80      0.36      0.50        11\n",
            "         243       0.00      0.00      0.00         2\n",
            "         244       0.00      0.00      0.00         0\n",
            "         245       0.00      0.00      0.00         2\n",
            "         246       0.00      0.00      0.00         1\n",
            "         247       0.00      0.00      0.00         0\n",
            "         248       0.00      0.00      0.00         1\n",
            "         249       0.20      0.20      0.20        15\n",
            "         250       0.00      0.00      0.00         0\n",
            "         251       0.18      0.33      0.23         9\n",
            "         252       0.12      1.00      0.22         1\n",
            "         253       0.00      0.00      0.00         1\n",
            "         254       0.00      0.00      0.00         2\n",
            "         255       0.00      0.00      0.00         1\n",
            "         256       0.55      0.86      0.67       980\n",
            "         257       0.00      0.00      0.00         0\n",
            "         258       0.00      0.00      0.00         7\n",
            "         259       0.00      0.00      0.00         2\n",
            "         260       0.00      0.00      0.00         0\n",
            "         261       0.00      0.00      0.00         3\n",
            "         262       0.00      0.00      0.00         0\n",
            "         263       0.00      0.00      0.00         2\n",
            "         264       0.00      0.00      0.00        10\n",
            "         265       0.00      0.00      0.00         1\n",
            "         266       0.00      0.00      0.00         3\n",
            "         267       0.00      0.00      0.00         2\n",
            "         268       0.46      0.64      0.54        72\n",
            "         269       0.00      0.00      0.00         4\n",
            "         270       0.00      0.00      0.00         0\n",
            "         271       0.39      0.28      0.33        57\n",
            "         272       0.00      0.00      0.00         0\n",
            "         273       0.00      0.00      0.00         0\n",
            "         274       0.00      0.00      0.00         0\n",
            "         275       0.53      0.56      0.55        16\n",
            "         276       0.00      0.00      0.00         3\n",
            "         277       0.00      0.00      0.00         1\n",
            "         278       0.61      0.68      0.65       145\n",
            "         279       0.00      0.00      0.00         2\n",
            "         280       0.00      0.00      0.00        13\n",
            "         281       0.00      0.00      0.00         1\n",
            "         282       0.00      0.00      0.00         0\n",
            "         283       0.00      0.00      0.00         1\n",
            "         284       0.00      0.00      0.00         3\n",
            "         285       0.00      0.00      0.00         3\n",
            "         286       0.00      0.00      0.00         0\n",
            "         287       0.00      0.00      0.00         1\n",
            "         288       0.00      0.00      0.00         9\n",
            "         289       0.00      0.00      0.00         1\n",
            "         290       0.00      0.00      0.00         1\n",
            "         291       0.00      0.00      0.00         1\n",
            "         292       0.00      0.00      0.00         3\n",
            "         293       0.00      0.00      0.00         2\n",
            "         294       0.00      0.00      0.00         0\n",
            "         295       0.27      0.27      0.27       230\n",
            "         296       0.79      0.52      0.63        21\n",
            "         297       0.00      0.00      0.00         0\n",
            "         298       0.00      0.00      0.00         2\n",
            "         299       0.00      0.00      0.00         3\n",
            "         300       0.00      0.00      0.00         2\n",
            "         301       0.00      0.00      0.00         0\n",
            "         302       0.00      0.00      0.00         3\n",
            "         303       0.00      0.00      0.00         3\n",
            "         304       0.00      0.00      0.00         1\n",
            "         305       0.00      0.00      0.00         0\n",
            "         306       0.00      0.00      0.00         2\n",
            "         307       0.38      0.14      0.20        22\n",
            "         308       0.00      0.00      0.00         1\n",
            "         309       0.39      0.77      0.52        61\n",
            "         310       0.00      0.00      0.00        17\n",
            "         311       0.00      0.00      0.00         0\n",
            "         312       0.00      0.00      0.00         6\n",
            "         313       0.00      0.00      0.00         0\n",
            "         314       0.80      0.40      0.53        10\n",
            "         315       0.00      0.00      0.00         1\n",
            "         316       0.00      0.00      0.00         1\n",
            "         317       0.00      0.00      0.00         0\n",
            "         318       0.00      0.00      0.00         0\n",
            "         319       0.16      0.27      0.20        15\n",
            "         320       0.00      0.00      0.00         1\n",
            "         321       0.34      0.65      0.45        23\n",
            "         322       0.00      0.00      0.00         0\n",
            "         323       0.00      0.00      0.00         2\n",
            "         324       0.00      0.00      0.00         7\n",
            "         325       1.00      0.19      0.32        16\n",
            "         326       0.25      0.07      0.11        14\n",
            "         327       0.00      0.00      0.00         2\n",
            "         328       0.00      0.00      0.00         1\n",
            "         329       0.00      0.00      0.00         0\n",
            "         330       0.00      0.00      0.00         1\n",
            "         331       0.00      0.00      0.00         2\n",
            "         332       0.00      0.00      0.00         4\n",
            "         333       0.00      0.00      0.00         7\n",
            "         334       0.00      0.00      0.00         7\n",
            "         335       0.00      0.00      0.00         5\n",
            "         336       0.00      0.00      0.00         0\n",
            "         337       0.00      0.00      0.00         1\n",
            "         338       0.00      0.00      0.00         9\n",
            "         339       0.00      0.00      0.00         1\n",
            "         340       0.69      0.43      0.53        63\n",
            "         341       0.00      0.00      0.00         0\n",
            "         342       0.00      0.00      0.00         0\n",
            "         343       0.00      0.00      0.00         2\n",
            "         344       0.00      0.00      0.00         1\n",
            "         345       0.00      0.00      0.00         1\n",
            "         346       0.00      0.00      0.00         0\n",
            "         347       0.43      0.19      0.26        16\n",
            "         348       0.00      0.00      0.00         1\n",
            "         349       0.00      0.00      0.00         0\n",
            "         350       0.00      0.00      0.00         2\n",
            "         351       0.00      0.00      0.00        12\n",
            "         352       0.00      0.00      0.00         0\n",
            "         353       0.00      0.00      0.00         3\n",
            "         354       0.00      0.00      0.00         3\n",
            "         355       0.00      0.00      0.00         4\n",
            "         356       0.00      0.00      0.00         0\n",
            "         357       0.00      0.00      0.00         0\n",
            "         358       0.00      0.00      0.00         4\n",
            "         359       0.00      0.00      0.00        17\n",
            "         360       0.00      0.00      0.00         2\n",
            "         361       0.00      0.00      0.00         0\n",
            "         362       0.00      0.00      0.00         0\n",
            "         363       0.00      0.00      0.00         0\n",
            "         364       0.00      0.00      0.00         2\n",
            "         365       0.00      0.00      0.00         2\n",
            "         366       0.00      0.00      0.00         1\n",
            "         367       0.00      0.00      0.00         1\n",
            "         368       0.00      0.00      0.00        15\n",
            "         369       0.00      0.00      0.00         1\n",
            "         370       0.00      0.00      0.00         2\n",
            "         371       0.00      0.00      0.00         3\n",
            "         372       0.45      0.35      0.39        74\n",
            "         373       0.00      0.00      0.00         1\n",
            "         374       0.00      0.00      0.00         0\n",
            "         375       0.00      0.00      0.00         0\n",
            "         376       0.36      0.26      0.30        19\n",
            "         377       0.00      0.00      0.00         4\n",
            "         378       0.00      0.00      0.00         1\n",
            "         379       0.00      0.00      0.00         1\n",
            "         380       0.00      0.00      0.00         0\n",
            "         381       0.00      0.00      0.00        11\n",
            "         382       0.00      0.00      0.00        12\n",
            "         383       0.00      0.00      0.00         0\n",
            "         384       0.00      0.00      0.00         0\n",
            "         385       0.00      0.00      0.00         1\n",
            "         386       0.00      0.00      0.00         8\n",
            "         387       0.00      0.00      0.00         0\n",
            "         388       0.00      0.00      0.00        13\n",
            "         389       0.20      0.33      0.25         6\n",
            "         390       0.33      0.30      0.32        23\n",
            "         391       0.00      0.00      0.00         7\n",
            "         392       0.00      0.00      0.00         1\n",
            "         393       0.00      0.00      0.00         3\n",
            "         394       0.00      0.00      0.00         0\n",
            "         395       0.00      0.00      0.00         1\n",
            "         396       0.60      1.00      0.75         9\n",
            "         397       0.00      0.00      0.00         1\n",
            "         398       0.00      0.00      0.00         5\n",
            "         399       0.00      0.00      0.00        13\n",
            "         400       0.00      0.00      0.00         0\n",
            "         401       0.00      0.00      0.00         3\n",
            "         402       0.00      0.00      0.00         1\n",
            "         403       0.00      0.00      0.00         8\n",
            "         404       0.00      0.00      0.00         1\n",
            "         405       0.00      0.00      0.00         1\n",
            "         406       0.00      0.00      0.00        59\n",
            "         407       0.00      0.00      0.00        16\n",
            "         408       0.00      0.00      0.00         1\n",
            "         409       0.47      0.82      0.60       604\n",
            "         410       0.00      0.00      0.00         3\n",
            "         411       0.00      0.00      0.00         1\n",
            "         412       0.00      0.00      0.00         0\n",
            "         413       0.42      0.87      0.57       580\n",
            "         414       0.00      0.00      0.00         0\n",
            "         415       0.00      0.00      0.00         2\n",
            "         416       0.00      0.00      0.00         5\n",
            "         417       0.00      0.00      0.00         0\n",
            "         418       0.00      0.00      0.00         1\n",
            "         419       0.00      0.00      0.00         9\n",
            "         420       0.00      0.00      0.00         7\n",
            "         421       0.00      0.00      0.00         1\n",
            "         422       0.00      0.00      0.00         3\n",
            "         423       0.00      0.00      0.00         4\n",
            "         424       0.00      0.00      0.00         1\n",
            "         425       0.00      0.00      0.00        15\n",
            "         426       0.00      0.00      0.00         0\n",
            "         427       0.00      0.00      0.00        17\n",
            "         428       0.21      0.43      0.29         7\n",
            "         429       0.00      0.00      0.00         0\n",
            "         430       0.00      0.00      0.00         0\n",
            "         431       0.00      0.00      0.00         3\n",
            "         432       0.00      0.00      0.00         6\n",
            "         433       0.43      0.79      0.56        38\n",
            "         434       0.00      0.00      0.00         3\n",
            "         435       0.00      0.00      0.00         3\n",
            "         436       0.00      0.00      0.00         2\n",
            "         437       0.00      0.00      0.00         2\n",
            "         438       0.00      0.00      0.00         5\n",
            "         439       0.00      0.00      0.00        18\n",
            "         440       0.00      0.00      0.00         0\n",
            "         441       0.00      0.00      0.00         2\n",
            "         442       0.00      0.00      0.00         7\n",
            "         443       0.00      0.00      0.00         1\n",
            "         444       0.00      0.00      0.00         2\n",
            "         445       0.00      0.00      0.00         2\n",
            "         446       0.00      0.00      0.00         0\n",
            "         447       0.00      0.00      0.00         0\n",
            "         448       0.00      0.00      0.00         2\n",
            "         449       0.24      0.48      0.32        31\n",
            "         450       0.66      0.46      0.54        46\n",
            "         451       0.00      0.00      0.00         0\n",
            "         452       0.00      0.00      0.00         1\n",
            "         453       0.91      0.93      0.92       321\n",
            "         454       0.00      0.00      0.00         0\n",
            "         455       0.00      0.00      0.00         0\n",
            "         456       0.00      0.00      0.00         1\n",
            "         457       0.00      0.00      0.00         0\n",
            "         458       0.00      0.00      0.00         0\n",
            "         459       0.00      0.00      0.00         4\n",
            "         460       0.00      0.00      0.00         2\n",
            "         461       0.00      0.00      0.00         1\n",
            "         462       0.00      0.00      0.00         8\n",
            "         463       0.00      0.00      0.00         1\n",
            "         464       0.00      0.00      0.00         0\n",
            "         465       0.00      0.00      0.00         6\n",
            "         466       0.61      0.62      0.61        37\n",
            "         467       0.00      0.00      0.00         2\n",
            "         468       0.00      0.00      0.00         1\n",
            "         469       0.00      0.00      0.00         3\n",
            "         470       0.00      0.00      0.00         0\n",
            "         471       0.00      0.00      0.00         0\n",
            "         472       0.00      0.00      0.00         3\n",
            "         473       0.00      0.00      0.00         9\n",
            "         474       0.00      0.00      0.00         7\n",
            "         475       1.00      0.12      0.21        17\n",
            "         476       0.75      0.19      0.30        16\n",
            "         477       0.00      0.00      0.00         0\n",
            "         478       0.00      0.00      0.00         5\n",
            "         479       0.25      0.18      0.21        11\n",
            "         480       0.00      0.00      0.00         1\n",
            "         481       0.00      0.00      0.00         0\n",
            "         482       0.37      0.57      0.45       159\n",
            "         483       0.00      0.00      0.00        22\n",
            "         484       0.00      0.00      0.00         0\n",
            "         485       0.00      0.00      0.00         2\n",
            "         486       0.00      0.00      0.00         0\n",
            "         487       0.00      0.00      0.00         1\n",
            "         488       0.00      0.00      0.00         0\n",
            "         489       0.29      0.08      0.12        51\n",
            "         490       0.00      0.00      0.00         0\n",
            "         491       0.00      0.00      0.00         3\n",
            "         492       0.31      0.80      0.45        20\n",
            "         493       0.33      0.24      0.28        21\n",
            "         494       0.00      0.00      0.00         8\n",
            "         495       0.52      0.70      0.60        87\n",
            "         496       0.52      0.53      0.52        64\n",
            "         497       0.00      0.00      0.00         0\n",
            "         498       0.67      0.81      0.73        27\n",
            "         499       0.00      0.00      0.00        14\n",
            "         500       0.00      0.00      0.00         9\n",
            "         501       0.00      0.00      0.00         1\n",
            "         502       0.00      0.00      0.00         1\n",
            "         503       0.00      0.00      0.00         2\n",
            "         504       0.44      0.54      0.48        13\n",
            "         505       0.00      0.00      0.00         2\n",
            "         506       0.00      0.00      0.00         1\n",
            "         507       0.00      0.00      0.00         0\n",
            "         508       0.00      0.00      0.00         3\n",
            "         509       0.00      0.00      0.00         0\n",
            "         510       0.00      0.00      0.00         1\n",
            "         511       0.00      0.00      0.00         5\n",
            "         512       0.00      0.00      0.00         4\n",
            "         513       0.00      0.00      0.00         0\n",
            "         514       0.20      0.05      0.08        94\n",
            "         515       0.52      0.77      0.62       262\n",
            "         516       0.53      0.88      0.66       795\n",
            "         517       0.85      0.80      0.83       164\n",
            "         518       0.46      0.89      0.60       661\n",
            "         519       0.62      0.41      0.49       156\n",
            "         520       0.00      0.00      0.00        27\n",
            "         521       0.10      0.03      0.05        31\n",
            "         522       0.00      0.00      0.00        21\n",
            "         523       0.82      0.51      0.62        79\n",
            "         524       0.84      0.58      0.69       156\n",
            "         525       0.00      0.00      0.00         6\n",
            "         526       0.00      0.00      0.00         5\n",
            "         527       0.00      0.00      0.00         0\n",
            "         528       0.00      0.00      0.00         1\n",
            "         529       0.58      0.91      0.71      1230\n",
            "         530       0.00      0.00      0.00         0\n",
            "         531       0.00      0.00      0.00         1\n",
            "         532       0.00      0.00      0.00         5\n",
            "         533       0.00      0.00      0.00         6\n",
            "         534       0.00      0.00      0.00         1\n",
            "         535       0.00      0.00      0.00         1\n",
            "         536       0.00      0.00      0.00         7\n",
            "         537       0.22      0.67      0.33         9\n",
            "         538       0.38      0.35      0.37        37\n",
            "         539       0.00      0.00      0.00        12\n",
            "         540       0.00      0.00      0.00         2\n",
            "         541       0.00      0.00      0.00         3\n",
            "         542       0.00      0.00      0.00         0\n",
            "         543       0.00      0.00      0.00         8\n",
            "         544       0.00      0.00      0.00         2\n",
            "         545       0.00      0.00      0.00        14\n",
            "         546       0.00      0.00      0.00         6\n",
            "         547       0.00      0.00      0.00         6\n",
            "         548       0.00      0.00      0.00         4\n",
            "         549       0.00      0.00      0.00        13\n",
            "         550       0.16      0.33      0.22        52\n",
            "         551       1.00      1.00      1.00         6\n",
            "         552       0.00      0.00      0.00         1\n",
            "         553       0.00      0.00      0.00         1\n",
            "         554       1.00      1.00      1.00         6\n",
            "         555       0.00      0.00      0.00         5\n",
            "         556       0.00      0.00      0.00         2\n",
            "         557       0.16      0.89      0.27        19\n",
            "         558       0.00      0.00      0.00         1\n",
            "         559       0.00      0.00      0.00         0\n",
            "         560       1.00      0.55      0.71        20\n",
            "         561       0.00      0.00      0.00         0\n",
            "         562       0.00      0.00      0.00         8\n",
            "         563       0.00      0.00      0.00        27\n",
            "         564       0.00      0.00      0.00         1\n",
            "         565       0.00      0.00      0.00        22\n",
            "         566       0.00      0.00      0.00         0\n",
            "         567       0.00      0.00      0.00         2\n",
            "         568       0.00      0.00      0.00         2\n",
            "         569       0.00      0.00      0.00         0\n",
            "         570       0.00      0.00      0.00         1\n",
            "         571       0.00      0.00      0.00         0\n",
            "         572       0.00      0.00      0.00         0\n",
            "         573       0.00      0.00      0.00         1\n",
            "         574       0.00      0.00      0.00         0\n",
            "         575       0.00      0.00      0.00        11\n",
            "         576       0.00      0.00      0.00         3\n",
            "         577       0.00      0.00      0.00         6\n",
            "         578       0.00      0.00      0.00         1\n",
            "         579       0.00      0.00      0.00        14\n",
            "         580       0.00      0.00      0.00         2\n",
            "         581       0.00      0.00      0.00         3\n",
            "         582       0.00      0.00      0.00         1\n",
            "         583       0.00      0.00      0.00         0\n",
            "         584       0.00      0.00      0.00        18\n",
            "         585       0.00      0.00      0.00         1\n",
            "         586       0.72      0.75      0.74       601\n",
            "         587       0.00      0.00      0.00        21\n",
            "         588       0.00      0.00      0.00         1\n",
            "         589       0.00      0.00      0.00        11\n",
            "         590       0.89      0.56      0.69        43\n",
            "         591       0.00      0.00      0.00         1\n",
            "         592       0.95      1.00      0.97      4489\n",
            "         593       0.00      0.00      0.00        33\n",
            "         594       0.45      0.45      0.45       393\n",
            "         595       0.00      0.00      0.00         0\n",
            "         596       0.00      0.00      0.00        98\n",
            "         597       0.00      0.00      0.00        21\n",
            "         598       0.00      0.00      0.00        14\n",
            "         599       0.29      0.30      0.30       380\n",
            "         600       0.71      0.78      0.74       599\n",
            "         601       0.00      0.00      0.00        58\n",
            "         602       0.00      0.00      0.00        20\n",
            "         603       0.33      0.45      0.38       405\n",
            "         604       0.83      0.99      0.90      3547\n",
            "         605       0.93      1.00      0.97      4424\n",
            "         606       0.71      0.92      0.81      2397\n",
            "         607       0.72      0.73      0.73       673\n",
            "         608       0.81      0.87      0.84      2355\n",
            "         609       0.51      0.89      0.65      1417\n",
            "         610       0.47      0.84      0.60       648\n",
            "         611       0.47      0.85      0.60       648\n",
            "         612       0.42      0.42      0.42       122\n",
            "         613       0.00      0.00      0.00        37\n",
            "         614       0.00      0.00      0.00         1\n",
            "         615       0.32      0.38      0.35       105\n",
            "         616       0.00      0.00      0.00         1\n",
            "         617       0.00      0.00      0.00        13\n",
            "         618       0.00      0.00      0.00         2\n",
            "         619       0.00      0.00      0.00        26\n",
            "         620       0.00      0.00      0.00         6\n",
            "         621       0.72      0.58      0.64       508\n",
            "         622       0.00      0.00      0.00        12\n",
            "         623       0.00      0.00      0.00         4\n",
            "         624       0.00      0.00      0.00        10\n",
            "         625       0.00      0.00      0.00         1\n",
            "         626       0.00      0.00      0.00        27\n",
            "         627       0.17      0.11      0.14       124\n",
            "         628       0.00      0.00      0.00         7\n",
            "         629       0.98      0.33      0.50       126\n",
            "         630       0.00      0.00      0.00         9\n",
            "         631       0.00      0.00      0.00        44\n",
            "         632       0.00      0.00      0.00         4\n",
            "         633       0.00      0.00      0.00        16\n",
            "         634       0.00      0.00      0.00         9\n",
            "         635       0.00      0.00      0.00        40\n",
            "         636       0.00      0.00      0.00         4\n",
            "         637       0.00      0.00      0.00        15\n",
            "         638       0.00      0.00      0.00         3\n",
            "         639       0.00      0.00      0.00         4\n",
            "         640       0.00      0.00      0.00        32\n",
            "         641       0.00      0.00      0.00         4\n",
            "         642       0.00      0.00      0.00        32\n",
            "         643       0.00      0.00      0.00        32\n",
            "         644       0.00      0.00      0.00         3\n",
            "         645       0.00      0.00      0.00         0\n",
            "         646       0.00      0.00      0.00        15\n",
            "         647       0.00      0.00      0.00         2\n",
            "         648       0.81      0.84      0.83       392\n",
            "         649       0.35      0.67      0.46        30\n",
            "         650       0.50      0.02      0.03        58\n",
            "         651       0.00      0.00      0.00         4\n",
            "         652       0.00      0.00      0.00        27\n",
            "         653       0.00      0.00      0.00        33\n",
            "         654       0.92      0.78      0.85       297\n",
            "         655       0.00      0.00      0.00         2\n",
            "         656       0.00      0.00      0.00        14\n",
            "         657       0.30      0.24      0.27       380\n",
            "         658       0.23      0.09      0.13       233\n",
            "         659       0.00      0.00      0.00         8\n",
            "         660       0.00      0.00      0.00         5\n",
            "         661       0.29      0.28      0.29       436\n",
            "         662       0.00      0.00      0.00         3\n",
            "         663       0.00      0.00      0.00         0\n",
            "         664       0.00      0.00      0.00         4\n",
            "         665       0.00      0.00      0.00         1\n",
            "         666       0.00      0.00      0.00         0\n",
            "         667       0.00      0.00      0.00         3\n",
            "         668       0.00      0.00      0.00        62\n",
            "         669       0.00      0.00      0.00         0\n",
            "         670       0.00      0.00      0.00         0\n",
            "         671       0.00      0.00      0.00         1\n",
            "         672       0.00      0.00      0.00        13\n",
            "         673       0.00      0.00      0.00         2\n",
            "         674       0.00      0.00      0.00         0\n",
            "         675       0.00      0.00      0.00         1\n",
            "         676       0.00      0.00      0.00         3\n",
            "         677       0.00      0.00      0.00         1\n",
            "         678       0.00      0.00      0.00         0\n",
            "         679       0.00      0.00      0.00         2\n",
            "         680       0.00      0.00      0.00         0\n",
            "         681       0.00      0.00      0.00         2\n",
            "         682       0.00      0.00      0.00         1\n",
            "         683       0.00      0.00      0.00        14\n",
            "         684       0.00      0.00      0.00         3\n",
            "         685       0.00      0.00      0.00         1\n",
            "         686       0.00      0.00      0.00         0\n",
            "         687       0.00      0.00      0.00         3\n",
            "         688       0.00      0.00      0.00         4\n",
            "         689       0.00      0.00      0.00         3\n",
            "\n",
            "   micro avg       0.69      0.76      0.72     50170\n",
            "   macro avg       0.10      0.10      0.10     50170\n",
            "weighted avg       0.66      0.76      0.69     50170\n",
            " samples avg       0.71      0.77      0.72     50170\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-neW68TjFQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}